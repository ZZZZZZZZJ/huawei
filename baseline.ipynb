{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "# matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option('display.width',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prepared step 1!\n"
     ]
    }
   ],
   "source": [
    "path = './data'\n",
    "data_train = pd.read_csv(path + \"/age_train.csv\",header=None)\n",
    "data_train.columns = ['uld','label']\n",
    "\n",
    "data_test = pd.read_csv(path + \"/age_test.csv\",header=None)\n",
    "data_test.columns = ['uld']\n",
    "data_test['label'] = -1\n",
    "\n",
    "data_all = pd.concat([data_train,data_test])\n",
    "print('data prepared step 1!')\n",
    "\n",
    "data_user_basic = pd.read_csv(path + \"/user_basic_info.csv\", header=None)\n",
    "user_basic_feature = ['uld','gender','city','prodName','ramCapacity','ramLeftRation','romCapacity','romLeftRation','color','fontSize','ct','carrier','os']\n",
    "data_user_basic.columns = user_basic_feature\n",
    "\n",
    "data_user_behavior = pd.read_csv(path + \"/user_behavior_info.csv\", header=None)\n",
    "user_behavior_feature = ['uld','bootTimes','AFuncTimes','BFuncTimes','CFuncTimes','DFuncTimes','EFuncTimes','FFuncTimes','FFuncSum']\n",
    "data_user_behavior.columns = user_behavior_feature\n",
    "\n",
    "temp  = pd.merge(data_all, data_user_basic,on='uld')\n",
    "data  = pd.merge(temp,data_user_behavior,on='uld')\n",
    "print('data prepared step 2!')\n",
    "cate_feat_list = user_basic_feature[1:] + user_behavior_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编码\n",
    "for i in cate_feat_list:\n",
    "    data[i] = data[i].map(dict(zip(data[i].unique(),range(0,data[i].nunique()))))\n",
    "\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "print('data prepared complete!')\n",
    "count_feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征计数\n",
    "def feature_count(data, features=[], is_feature=True):\n",
    "    if len(set(features)) != len(features):\n",
    "        print('equal feature !!!!')\n",
    "        return data\n",
    "    new_feature = 'count'\n",
    "    nunique = []\n",
    "    for i in features:\n",
    "        nunique.append(data[i].nunique())\n",
    "        new_feature += '_' + i.replace('add_','')\n",
    "    if len(features) > 1 and len(data[features].drop_duplicates()) <= np.max(nunique):\n",
    "        print(new_feature, 'is unvalid cross feature:')\n",
    "        return data\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "    data = data.merge(temp,'left',on=features)\n",
    "    if is_feature:\n",
    "        count_feature_list.append(new_feature)\n",
    "    return data\n",
    "\n",
    "for i in cate_feat_list:\n",
    "    n = data[i].nunique()\n",
    "    if n > 5:\n",
    "        data = feature_count(data,[i])\n",
    "\n",
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cate_feat_list + count_feature_list #采用基本特征+计数特征\n",
    "print(len(feature),feature)\n",
    "\n",
    "#print(data.head())\n",
    "\n",
    "#低频过滤\n",
    "for feature in cate_feat_list:\n",
    "    if 'count_' + feature in data.keys():\n",
    "        print(feature)\n",
    "        data.loc[data['count_'+feature]<2, feature] = -1\n",
    "        data[feature] = data[feature] + 1\n",
    "        \n",
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = data[(data.label == -1)]\n",
    "#print(predict.head())\n",
    "predict_result = predict[['uld']]\n",
    "pred_temp = predict[['uld']]\n",
    "#print(predict_result.head())\n",
    "predict_result['predicted_score'] = 0\n",
    "predict_x = predict.drop('label',axis=1)\n",
    "print(predict_x.columns.values)\n",
    "train_x = data[data.label != -1].reset_index(drop=True)\n",
    "train_y = train_x.pop('label').values\n",
    "print(train_y.size)\n",
    "base_train_csr = sparse.csr_matrix((len(train_x),0))\n",
    "base_predict_csr = sparse.csr_matrix((len(predict_x),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "for feature in cate_feat_list:\n",
    "    enc.fit(data[feature].values.reshape(-1,1))\n",
    "    base_train_csr = sparse.hstack((base_train_csr,enc.transform(train_x[feature].values.reshape(-1,1))),'csr','bool')\n",
    "    base_predict_csr = sparse.hstack((base_predict_csr,enc.transform(predict[feature].values.reshape(-1,1))),'csr','bool')\n",
    "print('one-hot prepared!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature = count_feature_list\n",
    "print(train_x.columns.values)\n",
    "train_csr = sparse.hstack((sparse.csr_matrix(train_x[num_feature]),base_train_csr),'csr').astype('float32')\n",
    "predict_csr = sparse.hstack((sparse.csr_matrix(predict_x[num_feature]),base_predict_csr),'csr').astype('float32')\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                               num_leaves=122, reg_alpha=3, reg_lambda=1,\n",
    "                               max_depth = -1, n_estimators=5000,objective='multiclass',num_class=6,\n",
    "                               subsample=0.8,colsample_bytree=0.8,subsample_feq=1,\n",
    "                               learning_rate=0.1,random_state=2018,n_jobs=10\n",
    "                              )\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018,shuffle=True)\n",
    "best_score = []\n",
    "print(train_y)\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train_csr,train_y)):\n",
    "    lgb_model.fit(train_csr[train_index], train_y[train_index],\n",
    "                 eval_set=[(train_csr[train_index],train_y[train_index]),\n",
    "                          (train_csr[test_index],train_y[test_index])],early_stopping_rounds=200, verbose=10)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['multi_logloss'])\n",
    "    print(best_score)\n",
    "    test_pred = lgb_model.predict(predict_csr)\n",
    "    print(test_pred)\n",
    "    pred_temp['label'] = test_pred\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.strftime('%m-%d-%H-%M')\n",
    "    pred_temp[['uld','label']].to_csv(path + \"/submission/lgb_baseline_split_%s.csv\" % now, index=False)\n",
    "    predict_result['predicted_score'] = predict_result['predicted_score'] + test_pred\n",
    "predict_result['predicted_score'] = predict_result['predicted_score'] / 5\n",
    "mean = predict_result['predicted_score'].mean()\n",
    "print('mean:',mean)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%m-%d-%H-%M')\n",
    "predict_result['label'] = predict_result['predicted_score']\n",
    "predict_result[['uld','label']].to_csv(path + \"/submission/lgb_baseline_%s.csv\" % now, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lgb_model,'lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
